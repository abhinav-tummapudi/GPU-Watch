import pandas as pd
import subprocess
from email_alert import send_warning_email
from logger import setup_logger
from utils import log_job_section
import numpy as np

logger = setup_logger()

def handle_warnings(df: pd.DataFrame, criterion: dict, dry_run: bool = False):
    # Preprocess
    df.replace(np.nan, 0, inplace = True)
    df['GPU_TYPE'] = df['GPU_ALLOCATED'].apply(lambda x: str(x).split(':')[0])
    df['THRESHOLD'] = df['GPU_TYPE'].map(lambda x: int(criterion['gpu_limit'].get(x, 0)))

    warning_jobs = df[
        (df['UTILIZATION_PERCENTAGE'] <= df['THRESHOLD']) &
        (df['TIME_SECONDS'] >= int(criterion['warning']['time']))
    ].copy()

    if warning_jobs.empty:
        logger.info("No underutilized jobs to warn.")
        return

    # Load previously warned jobs
    try:
        warned_prev = pd.read_csv('data/job_warned.csv')
        if warned_prev.empty or warned_prev.columns.size == 0:
            raise ValueError("Empty or malformed CSV")
    except (FileNotFoundError, pd.errors.EmptyDataError, ValueError):
        warned_prev = pd.DataFrame(columns=['SLURM_JOB_ID'])

    # Filter out jobs that were already warned
    new_warnings = warning_jobs[~warning_jobs['SLURM_JOB_ID'].isin(warned_prev['SLURM_JOB_ID'])].copy()

    if new_warnings.empty:
        logger.info("All underutilized jobs have already been warned.")
        return

    logger.info(f"Found {len(new_warnings)} *new* underutilized job(s) to warn")

    for _, row in new_warnings.iterrows():
        if dry_run:
            logger.info(f"[Dry Run] Would warn job {row['SLURM_JOB_ID']} user {row['USER']}")
        else:
            success = send_warning_email(
                user_netid=row['USER'],
                slurm_job_id=row['SLURM_JOB_ID'],
                gpu_allocated=row['GPU_ALLOCATED'],
                used_memory_mb=row['USED_MEMORY'],
                gpu_node=row['NODELIST(REASON)']
            )
            logger.info(f"Sent email to {row['USER']} for job {row['SLURM_JOB_ID']} ‚Üí {'‚úÖ' if success else '‚ùå'}")

    # Update job_warned.csv
    #if not dry_run:
    warned_combined = pd.concat([warned_prev, new_warnings], ignore_index=True).drop_duplicates('SLURM_JOB_ID')
    warned_combined.to_csv('data/job_warned.csv', index=False)
    logger.info("üìù Updated warned jobs file.")
    if not dry_run:
        # Append to human-readable audit log
        log_job_section("WARNING JOBS", new_warnings)


def handle_kills(df: pd.DataFrame, criterion: dict, dry_run: bool = False):
    df['GPU_TYPE'] = df['GPU_ALLOCATED'].apply(lambda x: str(x).split(':')[0])
    df['THRESHOLD'] = df['GPU_TYPE'].map(lambda x: int(criterion['gpu_limit'].get(x, 0)))

    kill_jobs = df[
        (df['UTILIZATION_PERCENTAGE'] <= df['THRESHOLD']) &
        (df['TIME_SECONDS'] >= int(criterion['kill']['time']))
    ].copy()

    if kill_jobs.empty:
        logger.info("No jobs to kill.")
        return

    logger.warning(f"Found {len(kill_jobs)} job(s) to be killed due to inefficiency")

    for _, row in kill_jobs.iterrows():
        job_id = row['SLURM_JOB_ID']
        if dry_run:
            logger.info(f"[Dry Run] Would cancel job {job_id}")
        else:
            try:
                subprocess.run(['scancel', str(job_id)], check=True)
                logger.critical(f"Cancelled job {job_id}")
            except Exception as e:
                logger.error(f"Failed to cancel job {job_id}: {e}")

    #if not dry_run:
    kill_jobs.to_csv('data/job_killed.csv', index=False)
    logger.info("Updated killed jobs file.")
    if not kill_jobs.empty and not dry_run:
        log_job_section("KILLED JOBS", kill_jobs)

